{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import heapq\n",
    "import random\n",
    "\n",
    "class BiGram:\n",
    "    def __init__(self, topk=10000):\n",
    "        self.i2w = {}\n",
    "        self.w2i = {}\n",
    "        self.M = None\n",
    "        self.topk = topk\n",
    "        return None\n",
    "    \n",
    "    def sort_dict(self,d):\n",
    "        sorted_d = sorted(d.items(), key = lambda item: item[1], reverse = True)\n",
    "        return sorted_d\n",
    "    \n",
    "    def train(self, trainset):\n",
    "        unkown = '<UKN>'\n",
    "        start = '<S>'\n",
    "        end = '<E>'\n",
    "        self.i2w[0]=unkown\n",
    "        self.i2w[1]=start\n",
    "        self.i2w[2]=end\n",
    "        totalcount = 0   #总词数\n",
    "        wordcount = defaultdict(int)  #词频\n",
    "        wordcount[start] = len(trainset)  #<s>句子开始标记数\n",
    "        wordcount[end] = len(trainset)   #<e>句子结束标记数\n",
    "        print(\"词频统计中...\")\n",
    "        #词频统计\n",
    "        for sentence in trainset:\n",
    "            for word in sentence:\n",
    "                wordcount[word]+=1\n",
    "        \n",
    "        #汇总总词数\n",
    "        for _,v in wordcount.items():\n",
    "            totalcount+=v\n",
    "        \n",
    "        \n",
    "        #选取topk高频词，同时获得 索引 与 词 的mapping \n",
    "        print(\"生成词表...\")\n",
    "        i = 3\n",
    "        for w,_ in self.sort_dict(wordcount):\n",
    "            if i < self.topk+3:\n",
    "                self.i2w[i]=w\n",
    "            i+=1\n",
    "        \n",
    "        #获得 词 与 索引 的mapping \n",
    "        for i,w in self.i2w.items():\n",
    "            self.w2i[w] = i\n",
    "        #print(self.w2i)\n",
    "        \n",
    "        #获得保留词的词频\n",
    "        wordcount_filter = defaultdict(int)\n",
    "        for word,v in wordcount.items():\n",
    "            #print(word)\n",
    "            if word in self.w2i:\n",
    "                wordcount_filter[self.w2i[word]]=v\n",
    "            else:\n",
    "                wordcount_filter[self.w2i['<UKN>']]+=v\n",
    "        #wordcount_filter[1]=len(trainset) #补充<s>\n",
    "        #wordcount_filter[2]=len(trainset)\n",
    "        \n",
    "        #print(wordcount_filter)\n",
    "                \n",
    "        voc_len = len(self.i2w)\n",
    "        \n",
    "        print(\"计算BiGram概率...\")\n",
    "        self.M = np.zeros((voc_len,voc_len)) #初始化概率矩阵\n",
    "        \n",
    "        #统计二元组合出现次数\n",
    "        print(\"统计二元组出现次数...\")\n",
    "        for sentence in trainset:\n",
    "            sentence_full = ['<S>'] + sentence + ['<E>']\n",
    "            sentence_idx = []\n",
    "            for word in sentence_full:\n",
    "                if word not in self.w2i:\n",
    "                    sentence_idx.append(self.w2i['<UKN>'])\n",
    "                else:\n",
    "                    sentence_idx.append(self.w2i[word])\n",
    "\n",
    "            for i in range(len(sentence_idx)-1):\n",
    "                p1 = sentence_idx[i]\n",
    "                p2 = sentence_idx[i+1]\n",
    "                self.M[p1][p2]+=1\n",
    "        \n",
    "        #计算二元组条件概率\n",
    "        print(\"计算二元组条件概率...\")\n",
    "        for i in range(voc_len):\n",
    "            for j in range(voc_len):\n",
    "                #print(self.M[i][j]+1, wordcount_filter[i], (self.M[i][j]+1)*wordcount_filter[i], (wordcount_filter[i]+wordlist_len))\n",
    "                self.M[i][j]=(1.0*(self.M[i][j]+1)*wordcount_filter[i])/(wordcount_filter[i]+voc_len)\n",
    "                #if self.M[i][j]!=0:\n",
    "                #    print(self.M[i][j])\n",
    "        #print(self.M)\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    def perplexity(self, testset):\n",
    "        '''\n",
    "        计算困惑度\n",
    "        '''\n",
    "        sent_perp = [] #存储每句话的困惑度数值\n",
    "        for sentence in testset:\n",
    "            #句子序列添加句首和句尾标签，并将词转换成索引序列\n",
    "            sentence_full = ['<S>'] + sentence + ['<E>']\n",
    "            sentence_idx = []\n",
    "            for word in sentence_full:\n",
    "                if word not in self.w2i:\n",
    "                    sentence_idx.append(self.w2i['<UKN>'])\n",
    "                else:\n",
    "                    sentence_idx.append(self.w2i[word])\n",
    "            #计算每个句子的联合概率\n",
    "            prob = 1\n",
    "            for i in range(len(sentence_idx)-1):\n",
    "                p1 = sentence_idx[i]\n",
    "                p2 = sentence_idx[i+1]\n",
    "                #print(self.M[p1][2])\n",
    "                prob*=1/self.M[p1][2]\n",
    "            \n",
    "            sent_perp.append(np.power(prob,1/len(sentence_full)))#困惑度计算\n",
    "        return np.mean(sent_perp)#返回句子困惑度的均值\n",
    "            \n",
    "    def infer(self):\n",
    "        '''\n",
    "        生成语句\n",
    "        '''\n",
    "        word_sequence = []\n",
    "        #根据句首标签生成下一个位置的词\n",
    "        start_idx = self.w2i['<S>'] \n",
    "        prob_list = self.M[start_idx].tolist()\n",
    "        #print(prob_list[:100])\n",
    "        #选择条件概率最大的前五个候选词，从中随机选择一个，增加多样性\n",
    "        top5_idx = heapq.nlargest(5,range(len(prob_list)),prob_list.__getitem__)\n",
    "        word_idx = top5_idx[random.randint(0,4)]\n",
    "        #print(word_idx,self.i2w[word_idx])\n",
    "        word_sequence.append(self.i2w[word_idx])\n",
    "        start_idx = word_idx\n",
    "        #直到遇到句尾标记停止，否则循环继续\n",
    "        while(word_idx!=self.w2i['<E>']):\n",
    "            prob_list = self.M[start_idx].tolist()\n",
    "            #print(prob_list[:100])\n",
    "            top5_idx = heapq.nlargest(5,range(len(prob_list)),prob_list.__getitem__)\n",
    "            word_idx = top5_idx[random.randint(0,4)]\n",
    "            #print(word_idx,self.i2w[word_idx])\n",
    "            word_sequence.append(self.i2w[word_idx])\n",
    "            start_idx = word_idx\n",
    "        return ' '.join(word_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总句子数： 19753\n",
      "词频统计中...\n",
      "生成词表...\n",
      "计算BiGram概率...\n",
      "统计二元组出现次数...\n",
      "计算二元组条件概率...\n",
      "已完成BiGram构建...\n",
      "BiGram困惑度： 23.932798554278946\n",
      "生成的句子 <UKN> ， 是 一 次 会议 <E>\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "punctions = ['！','。','?']\n",
    "with open('199801.txt',\"r\",encoding=\"utf-8\") as f:\n",
    "    c = 0\n",
    "    for line in f:\n",
    "        if c>10000:\n",
    "            break\n",
    "        c+=1\n",
    "        ls = line.strip().split('  ')\n",
    "        line_segs = []\n",
    "        candidate_sentence = []\n",
    "        w_num = 0\n",
    "        for i in range(1,len(ls)):\n",
    "            if ls[i].endswith('/w') and ls[i][0] in punctions:\n",
    "                w_num+=1\n",
    "                if len(candidate_sentence)>=1:\n",
    "                    sentences.append(candidate_sentence)\n",
    "                    candidate_sentence = []\n",
    "            else:\n",
    "                end_idx = ls[i].index('/')\n",
    "                candidate_sentence.append(ls[i][:end_idx])\n",
    "        if len(candidate_sentence)>0:\n",
    "            sentences.append(candidate_sentence)\n",
    "print(\"总句子数：\",len(sentences))\n",
    "\n",
    "split_rate = 0.8 \n",
    "topk = 10000\n",
    "\n",
    "bigram = BiGram(topk)\n",
    "\n",
    "split_rate = int(split_rate*len(sentences))\n",
    "bigram.train(sentences[:split_rate])\n",
    "print('已完成BiGram构建...')\n",
    "\n",
    "print('BiGram困惑度：',bigram.perplexity(sentences[split_rate:]))\n",
    "\n",
    "#print(bigram.M[0][0])\n",
    "sentence = bigram.infer() \n",
    "print('生成的句子',sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总句子数： 44473\n",
      "词频统计中...\n",
      "生成词表...\n",
      "计算BiGram概率...\n",
      "统计二元组出现次数...\n",
      "计算二元组条件概率...\n",
      "已完成BiGram构建...\n",
      "BiGram困惑度： 13.342639884517247\n",
      "生成的句子 ” ， 是 <UKN> 的 ， <UKN> <E>\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "punctions = ['！','。','?','']\n",
    "with open('199801.txt',\"r\",encoding=\"utf-8\") as f:\n",
    "    #c = 0\n",
    "    for line in f:\n",
    "        #if c>10000:\n",
    "        #    break\n",
    "        #c+=1\n",
    "        ls = line.strip().split('  ')\n",
    "        line_segs = []\n",
    "        candidate_sentence = []\n",
    "        w_num = 0\n",
    "        for i in range(1,len(ls)):\n",
    "            if ls[i].endswith('/w') and ls[i][0] in punctions:\n",
    "                w_num+=1\n",
    "                if len(candidate_sentence)>=1:\n",
    "                    sentences.append(candidate_sentence)\n",
    "                    candidate_sentence = []\n",
    "            else:\n",
    "                end_idx = ls[i].index('/')\n",
    "                candidate_sentence.append(ls[i][:end_idx])\n",
    "        if len(candidate_sentence)>0:\n",
    "            sentences.append(candidate_sentence)\n",
    "print(\"总句子数：\",len(sentences))\n",
    "\n",
    "split_rate = 0.8 \n",
    "topk = 10000\n",
    "\n",
    "bigram = BiGram(topk)\n",
    "\n",
    "split_rate = int(split_rate*len(sentences))\n",
    "bigram.train(sentences[:split_rate])\n",
    "print('已完成BiGram构建...')\n",
    "\n",
    "print('BiGram困惑度：',bigram.perplexity(sentences[split_rate:]))\n",
    "\n",
    "#print(bigram.M[0][0])\n",
    "sentence = bigram.infer() \n",
    "print('生成的句子:',sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的句子: 在 新 问题 是 <UKN> ， 并 不 能 使 人 <E>\n"
     ]
    }
   ],
   "source": [
    "sentence = bigram.infer() \n",
    "print('生成的句子:',sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\zhang\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.805 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['It', ' ', 'is', ' ', 'good', '/']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.lcut(\"It is good/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
