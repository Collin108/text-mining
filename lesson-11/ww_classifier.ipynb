{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Pytorch实现多层神经网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据\n",
    "\n",
    "任务：给一句话中的每个单词打标签，将表示地点（location）的单词标记为1，将其它单词标记为0。\n",
    "\n",
    "只考虑词长为1的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练数据集\n",
    "train_sents = [s.lower().split() for s in [\"we 'll always have Paris\",\n",
    "                                           \"I live in Germany\",\n",
    "                                           \"He comes from Denmark\",\n",
    "                                           \"The capital of Denmark is Copenhagen\"]]\n",
    "train_labels = [[0, 0, 0, 0, 1],\n",
    "                [0, 0, 0, 1],\n",
    "                [0, 0, 0, 1],\n",
    "                [0, 0, 0, 1, 0, 1]]\n",
    "\n",
    "#确认训练数据的长度与对应的标签数据是否一致。\n",
    "assert all([len(train_sents[i]) == len(train_labels[i]) for i in range(len(train_sents))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试数据集\n",
    "test_sents = [s.lower().split() for s in [\"She comes from Paris\"]]\n",
    "test_labels = [[0, 0, 0, 1]]\n",
    "\n",
    "assert all([len(test_sents[i]) == len(test_labels[i]) for i in range(len(test_sents))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建数据的分批张量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch 深度学习框架，可以针对张量（tensor）进行优化学习，这里的张量可以理解为任意维度的向量（vectors）和矩阵（matrices）。\n",
    "\n",
    "接下来，将介绍把数据转换为词典索引的list以及构建分批张量（batch tensors）的方法。（分批的数据是模型的简便的输入形式）。\n",
    "\n",
    "使用*torch.utils.data.DataLoader*对象处理批次数次和迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把分词后的句子list转换为词典索引。\n",
    "\n",
    "假设有以下词典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id_2_word使用list存储词典，顺序即表示词在词典中的索引。\n",
    "id_2_word = [\"<pad>\", \"<unk>\", \"we\", \"always\", \"have\", \"paris\",\n",
    "              \"i\", \"live\", \"in\", \"germany\",\n",
    "              \"he\", \"comes\", \"from\", \"denmark\",\n",
    "              \"the\", \"of\", \"is\", \"copenhagen\"]\n",
    "\n",
    "#word_2_id 使用dict表示词和词索引（word,word_indice）\n",
    "word_2_id = {w:i for i,w in enumerate(id_2_word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', \"'ll\", 'always', 'have', 'paris']\n"
     ]
    }
   ],
   "source": [
    "#打印第一句话\n",
    "instance = train_sents[0]\n",
    "print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义方法，将词（token）转换为在词典中的索引（indice）\n",
    "def convert_tokens_to_inds(sentence, word_2_id):\n",
    "    #获取每个词对应的id（索引），对于word_2_id中不存在的词使用“<unk>”的id替换。\n",
    "    return [word_2_id.get(t, word_2_id[\"<unk>\"]) for t in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "#打印一条替换成索引后的样本数据，也即使用词典编码后的序列。\n",
    "token_inds = convert_tokens_to_inds(instance, word_2_id)\n",
    "pp.pprint(token_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', '<unk>', 'always', 'have', 'paris']\n"
     ]
    }
   ],
   "source": [
    "#将使用索引编码的语句还原成单词表示，也即解码\n",
    "print([id_2_word[tok_idx] for tok_idx in token_inds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对窗口进行填充。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在词窗口分类问题中，对于语句中的每个单词需要使用其左右两侧的n个单词。\n",
    "\n",
    "为了使对任意单词的取上下文词操作都可以正常进行，需要在语句的开头之前和结尾之后进行填充。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#句首句尾填充方法，使用\"<pad>\"填充。\n",
    "def pad_sentence_for_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "    return [pad_token]*window_size + sentence + [pad_token]*window_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<pad>', 'we', \"'ll\", 'always', 'have', 'paris', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "#假定窗口大小为2，打印首位填充后的语句样例。\n",
    "window_size = 2\n",
    "instance = pad_sentence_for_window(train_sents[0], window_size)\n",
    "print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试验证在给定词典上的编码和解码操作没有问题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<pad>', 'we', '<unk>', 'always', 'have', 'paris', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', 'i', 'live', 'in', 'germany', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', 'he', 'comes', 'from', 'denmark', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', 'the', '<unk>', 'of', 'denmark', 'is', 'copenhagen', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "for sent in train_sents:\n",
    "    #将原始语句序列填充后转换为词典索引序列\n",
    "    tok_idxs = convert_tokens_to_inds(pad_sentence_for_window(sent, window_size), word_2_id)\n",
    "    #将词典索引序列还原为词序列\n",
    "    print([id_2_word[idx] for idx in tok_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用DataLoader将语句批次化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练模型时，因为单条样本可能会对全局loss梯度的计算引入明显的噪声，很少每次针对一条单独的训练数据更新模型参数。因此，通常的做法时构建小规模的批数据（batches），针对每批数据更新参数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定batch大小，针对使用词典索引编码的样本数据进行分批化处理。\n",
    "\n",
    "对于每个包含B条样本的输入，进行以下操作：\n",
    "\n",
    "（1）对batch中的每条数据进行首尾填充。\n",
    "\n",
    "（2）对batch中的每条数据的末尾进行额外填充，使得batch中的每条数据长度相等。\n",
    "\n",
    "（3）将样本标签转换成预先设定的格式。\n",
    "\n",
    "对于数据集：\n",
    "\n",
    "（4）对每轮（epoch）训练，对整个训练数据进行shuffle操作。\n",
    "\n",
    "（5）保证对输入和标签的shuffle同步进行，样本的相对顺序保持一一对应。\n",
    "\n",
    "Pytorch提供了对象*torch.utils.data.DataLoader*，可以实现（4）和（5），对于（1），（2），（3），需要自行实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('raw train label instance', tensor([0, 0, 0, 0, 1]))\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "#将训练数据类别向量转换成一个LongTensor，打印张量的内容以及size。\n",
    "\n",
    "l = torch.LongTensor(train_labels[0])\n",
    "pp.pprint((\"raw train label instance\", l))\n",
    "print(l.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('unfilled label instance',\n",
      " tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]]))\n",
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "#声明一个2*len(l)的0张量，并打印其内容和维度（size）。\n",
    "one_hots = torch.zeros((2, len(l)))\n",
    "pp.pprint((\"unfilled label instance\", one_hots))\n",
    "print(one_hots.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('one-hot labels', tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.]]))\n"
     ]
    }
   ],
   "source": [
    "#对声明的0张量进行替换操作，转换成one-hot形式\n",
    "one_hots[1] = l\n",
    "pp.pprint((\"one-hot labels\", one_hots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('one-hot labels', tensor([[1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]]))\n"
     ]
    }
   ],
   "source": [
    "l_not = 1-l.byte()#0和1比特位转换\n",
    "one_hots[0] = l_not\n",
    "pp.pprint((\"one-hot labels\", one_hots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(data, window_size, word_2_id):\n",
    "    \"\"\"\n",
    "    对于一堆句子和标签，进行以下操作：\n",
    "    -窗口填充（padding），\n",
    "    -语句长度填充，\n",
    "    -将类别标签转换成one-hot向量\n",
    "    -返回填充后的输入，one-hot标签以及对应的长度\n",
    "    \"\"\"\n",
    "    \n",
    "    x_s, y_s = zip(*data)#将data中的数据拆解成压缩前的形式\n",
    "\n",
    "    # 窗口填充\n",
    "    window_padded = [convert_tokens_to_inds(pad_sentence_for_window(sentence, window_size), word_2_id)\n",
    "                                                                                  for sentence in x_s]\n",
    "    # 等长填充\n",
    "    padded = nn.utils.rnn.pad_sequence([torch.LongTensor(t) for t in window_padded], batch_first=True)\n",
    "    \n",
    "    # 将类别标签转换成one-hot形式，使用1标识原始语句中有单词的位置，使用0表示填充的位置\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    for y in y_s:\n",
    "        lengths.append(len(y))\n",
    "        label = torch.zeros((len(y),2 ))\n",
    "        true = torch.LongTensor(y) \n",
    "        false = 1-true.byte()\n",
    "        label[:, 0] = false\n",
    "        label[:, 1] = true\n",
    "        labels.append(label)\n",
    "    padded_labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "    \n",
    "    return padded.long(), padded_labels, torch.LongTensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle=True时对训练阶段的数据加载是有用的。\n",
    "# 使用偏函数functools.partial构建一个指定了部分参数的方法。\n",
    "\n",
    "example_loader = DataLoader(list(zip(train_sents, \n",
    "                                                      train_labels)), \n",
    "                                             batch_size=2, \n",
    "                                             shuffle=True, \n",
    "                                             collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('inputs',\n",
      " tensor([[0, 0, 6, 7, 8, 9, 0, 0, 0],\n",
      "        [0, 0, 2, 1, 3, 4, 5, 0, 0]]),\n",
      " torch.Size([2, 9]))\n",
      "('labels',\n",
      " tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]]),\n",
      " torch.Size([2, 5, 2]))\n",
      "tensor([4, 5])\n"
     ]
    }
   ],
   "source": [
    "#对以上创建的数据加载方法进行测试：\n",
    "for batched_input, batched_labels, batch_lengths in example_loader:\n",
    "    pp.pprint((\"inputs\", batched_input, batched_input.size()))\n",
    "    pp.pprint((\"labels\", batched_labels, batched_labels.size()))\n",
    "    pp.pprint(batch_lengths)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型\n",
    "\n",
    "### 词窗口的向量化\n",
    "\n",
    "对于输入的每批语句中的每条语句i的每个单词j，我们需要基于其左右的上下文词构建一个张量表示这个单词j。\n",
    "\n",
    "因此，我们需要一个维度为(B,L,2N+1)的词索引矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成模拟的样本数据，使用以下数据迭代方法构建分批输入，对未填充的词索引序列构建窗口："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1, 2, 3, 4, 0, 0],\n",
      "        [0, 0, 5, 6, 7, 8, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "#生成一个模拟输入\n",
    "dummy_input = torch.zeros(2, 8).long()\n",
    "dummy_input[:,2:-2] = torch.arange(1,9).view(2,4)\n",
    "pp.pprint(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 5])\n",
      "tensor([[[0, 0, 1, 2, 3],\n",
      "         [0, 1, 2, 3, 4],\n",
      "         [1, 2, 3, 4, 0],\n",
      "         [2, 3, 4, 0, 0]],\n",
      "\n",
      "        [[0, 0, 5, 6, 7],\n",
      "         [0, 5, 6, 7, 8],\n",
      "         [5, 6, 7, 8, 0],\n",
      "         [6, 7, 8, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "#从输入数据中获取窗口对应的所有情况\n",
    "\n",
    "dummy_output = [[[dummy_input[i, j-2+k].item() for k in range(2*2+1)] \n",
    "                                                     for j in range(2, 6)] \n",
    "                                                            for i in range(2)]\n",
    "dummy_output = torch.LongTensor(dummy_output)\n",
    "print(dummy_output.size())\n",
    "pp.pprint(dummy_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每批数据中的每条数据，对于每个原始语句中的每个单词，忽略窗口填充，可以得到中心词左右的5个词索引，但在实际应用中，这种操作很慢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为更好的实现，pytorch的张量运算__Tensor.unfold__可以简便高效地实现这种操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 1, 2, 3],\n",
       "         [0, 1, 2, 3, 4],\n",
       "         [1, 2, 3, 4, 0],\n",
       "         [2, 3, 4, 0, 0]],\n",
       "\n",
       "        [[0, 0, 5, 6, 7],\n",
       "         [0, 5, 6, 7, 8],\n",
       "         [5, 6, 7, 8, 0],\n",
       "         [6, 7, 8, 0, 0]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input.unfold(1, 2*2+1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完整模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用pytorch，我们通过扩展nn.Module类实现模型。最简洁的，需要实现*\\_\\_init\\_\\_* 和*forward* function两个方法。\n",
    "\n",
    "\n",
    "在*\\_\\_init\\_\\_*中存储模型参数（权重）和超参数（维度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWordWindowClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    包含一层隐藏层的基于窗口的词二元分类模型\n",
    "    \"\"\"\n",
    "    def __init__(self, config, vocab_size, pad_idx=0):\n",
    "        super(SoftmaxWordWindowClassifier, self).__init__()\n",
    "        \"\"\"\n",
    "        Instance variables.\n",
    "        \"\"\"\n",
    "        self.window_size = 2*config[\"half_window\"]+1\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        self.freeze_embeddings = config[\"freeze_embeddings\"]\n",
    "        \n",
    "        \"\"\"\n",
    "        Embedding layer，词向量层\n",
    "        -model holds an embedding for each layer in our vocab\n",
    "        -sets aside a special index in the embedding matrix for padding vector (of zeros)\n",
    "        -by default, embeddings are parameters (so gradients pass through them)\n",
    "        \"\"\"\n",
    "        self.embed_layer = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_idx)\n",
    "        if self.freeze_embeddings:\n",
    "            self.embed_layer.weight.requires_grad = False\n",
    "        \n",
    "        \"\"\"\n",
    "        Hidden layer，隐藏层\n",
    "        -we want to map embedded word windows of dim (window_size+1)*self.embed_dim to a hidden layer.\n",
    "        -nn.Sequential allows you to efficiently specify sequentially structured models\n",
    "            -first the linear transformation is evoked on the embedded word windows\n",
    "            -next the nonlinear transformation tanh is evoked.\n",
    "        \"\"\"\n",
    "        self.hidden_layer = nn.Sequential(nn.Linear(self.window_size*self.embed_dim, \n",
    "                                                    self.hidden_dim), \n",
    "                                          nn.Tanh())\n",
    "        \n",
    "        \"\"\"\n",
    "        Output layer，输出层\n",
    "        -we want to map elements of the output layer (of size self.hidden dim) to a number of classes.\n",
    "        \"\"\"\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, self.num_classes)\n",
    "        \n",
    "        \"\"\"\n",
    "        Softmax，softmax分类层\n",
    "        -The final step of the softmax classifier: mapping final hidden layer to class scores.\n",
    "        -pytorch has both logsoftmax and softmax functions (and many others)\n",
    "        -since our loss is the negative LOG likelihood, we use logsoftmax\n",
    "        -technically you can take the softmax, and take the log but PyTorch's implementation\n",
    "         is optimized to avoid numerical underflow issues.\n",
    "        \"\"\"\n",
    "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Let B:= batch_size\n",
    "            L:= window-padded sentence length\n",
    "            D:= self.embed_dim\n",
    "            S:= self.window_size\n",
    "            H:= self.hidden_dim\n",
    "            \n",
    "        inputs: a (B, L) tensor of token indices\n",
    "        \"\"\"\n",
    "        B, L = inputs.size()\n",
    "        \n",
    "        \"\"\"\n",
    "        Reshaping.\n",
    "        Takes in a (B, L) LongTensor\n",
    "        Outputs a (B, L~, S) LongTensor\n",
    "        \"\"\"\n",
    "        # Fist, get our word windows for each word in our input.\n",
    "        token_windows = inputs.unfold(1, self.window_size, 1)\n",
    "        _, adjusted_length, _ = token_windows.size()\n",
    "        \n",
    "        # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
    "        assert token_windows.size() == (B, adjusted_length, self.window_size)\n",
    "        \n",
    "        \"\"\"\n",
    "        Embedding.\n",
    "        Takes in a torch.LongTensor of size (B, L~, S) \n",
    "        Outputs a (B, L~, S, D) FloatTensor.\n",
    "        \"\"\"\n",
    "        embedded_windows = self.embed_layer(token_windows)\n",
    "        \n",
    "        \"\"\"\n",
    "        Reshaping.\n",
    "        Takes in a (B, L~, S, D) FloatTensor.\n",
    "        Resizes it into a (B, L~, S*D) FloatTensor.\n",
    "        -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
    "        \"\"\"\n",
    "        embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
    "        \n",
    "        \"\"\"\n",
    "        Layer 1.\n",
    "        Takes in a (B, L~, S*D) FloatTensor.\n",
    "        Resizes it into a (B, L~, H) FloatTensor\n",
    "        \"\"\"\n",
    "        layer_1 = self.hidden_layer(embedded_windows)\n",
    "        \n",
    "        \"\"\"\n",
    "        Layer 2\n",
    "        Takes in a (B, L~, H) FloatTensor.\n",
    "        Resizes it into a (B, L~, 2) FloatTensor.\n",
    "        \"\"\"\n",
    "        output = self.output_layer(layer_1)\n",
    "        \n",
    "        \"\"\"\n",
    "        Softmax.\n",
    "        Takes in a (B, L~, 2) FloatTensor of unnormalized class scores.\n",
    "        Outputs a (B, L~, 2) FloatTensor of (log-)normalized class scores.\n",
    "        \"\"\"\n",
    "        output = self.log_softmax(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练\n",
    "\n",
    "使用以上设计的模型进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#损失函数\n",
    "\n",
    "def loss_function(outputs, labels, lengths):\n",
    "    \"\"\"对每批模型的预测结果计算负对数损失值\"\"\"\n",
    "    B, L, num_classes = outputs.size()\n",
    "    num_elems = lengths.sum().float()\n",
    "        \n",
    "    # get only the values with non-zero labels\n",
    "    loss = outputs*labels\n",
    "    \n",
    "    # rescale average\n",
    "    return -loss.sum() / num_elems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(loss_function, optimizer, model, train_data):\n",
    "    \n",
    "    # 对每批数据，必须重置模型中存储的梯度\n",
    "    total_loss = 0\n",
    "    for batch, labels, lengths in train_data:\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        # evoke model in training mode on batch\n",
    "        outputs = model.forward(batch)\n",
    "        # compute loss w.r.t batch\n",
    "        loss = loss_function(outputs, labels, lengths)\n",
    "        # pass gradients back, startiing on loss value\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # return the total to keep track of how you did this time around\n",
    "    return total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"batch_size\": 4,\n",
    "          \"half_window\": 2,\n",
    "          \"embed_dim\": 25,\n",
    "          \"hidden_dim\": 25,\n",
    "          \"num_classes\": 2,\n",
    "          \"freeze_embeddings\": False,\n",
    "         }\n",
    "learning_rate = .0002\n",
    "num_epochs = 10000\n",
    "model = SoftmaxWordWindowClassifier(config, len(word_2_id))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练数据加载器\n",
    "train_loader = torch.utils.data.DataLoader(list(zip(train_sents, train_labels)), \n",
    "                                           batch_size=2, \n",
    "                                           shuffle=True, \n",
    "                                           collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5760300159454346, 1.5034716129302979, 1.428852379322052, 1.3606516122817993, 1.294142723083496, 1.2307620644569397, 1.1729514002799988, 1.1177958846092224, 1.064444661140442, 1.0144432187080383, 0.9647728204727173, 0.9145443737506866, 0.8714427649974823, 0.827903687953949, 0.782893031835556, 0.7464790642261505, 0.7085845470428467, 0.672574907541275, 0.6383797824382782, 0.6020388603210449, 0.5740203559398651, 0.5464770495891571, 0.5153871178627014, 0.49353277683258057, 0.4680628776550293, 0.4465492516756058, 0.4240477979183197, 0.40504635870456696, 0.386140376329422, 0.36748890578746796, 0.34894420206546783, 0.3352568596601486, 0.32057951390743256, 0.3050563931465149, 0.29379647970199585, 0.2799808233976364, 0.27057111263275146, 0.2577608972787857, 0.24760308861732483, 0.23938872665166855, 0.23030203580856323, 0.2217102125287056, 0.213837131857872, 0.20592142641544342, 0.1975209042429924, 0.19177205860614777, 0.1852521151304245, 0.17805130779743195, 0.17332740873098373, 0.16762225329875946, 0.16242052614688873, 0.156401127576828, 0.15248671174049377, 0.14791759848594666, 0.14276819676160812, 0.13864275813102722, 0.13549645990133286, 0.1309451162815094, 0.12805786728858948, 0.12391772121191025, 0.12130759283900261, 0.1174844540655613, 0.11509734764695168, 0.11158104613423347, 0.10880914703011513, 0.10672499239444733, 0.10415618121623993, 0.1016879603266716, 0.09931637346744537, 0.09651747718453407, 0.0943353958427906, 0.0922357365489006, 0.09021246433258057, 0.08873026445508003, 0.08682137727737427, 0.08501465991139412, 0.08323439210653305, 0.08155384659767151, 0.07991182059049606, 0.07832476496696472, 0.07679140195250511, 0.07530859857797623, 0.0734948217868805, 0.07211398333311081, 0.07077760994434357, 0.06981953233480453, 0.06855795159935951, 0.06733452901244164, 0.065834054723382, 0.06499703787267208, 0.06358030252158642, 0.0627965684980154, 0.06145665422081947, 0.060722390189766884, 0.05945282615721226, 0.058493006974458694, 0.0578459445387125, 0.05691223032772541, 0.05602281913161278, 0.055180735886096954]\n"
     ]
    }
   ],
   "source": [
    "#训练每迭代100次，记录一次模型损失值\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = train_epoch(loss_function, optimizer, model, train_loader)\n",
    "    if epoch % 100 == 0:\n",
    "        losses.append(epoch_loss)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试数据加载器\n",
    "test_loader = torch.utils.data.DataLoader(list(zip(test_sents, test_labels)), \n",
    "                                           batch_size=1, \n",
    "                                           shuffle=False, \n",
    "                                           collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 1]])\n",
      "tensor([[0, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "for test_instance, labels, _ in test_loader:\n",
    "    outputs = model.forward(test_instance)\n",
    "    print(torch.argmax(outputs, dim=2))\n",
    "    print(torch.argmax(labels, dim=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text classification using torchText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Downloading https://files.pythonhosted.org/packages/e2/cc/286543384fef54588c7824803c296cbd0fa2338fb82292c5b9a35b1c96c8/torchtext-0.9.1-cp36-cp36m-win_amd64.whl (1.3MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from torchtext) (4.52.0)\n",
      "Requirement already satisfied: requests in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from torchtext) (2.25.1)\n",
      "Requirement already satisfied: torch==1.8.1 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from torchtext) (1.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from torchtext) (1.19.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from requests->torchtext) (2020.11.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from requests->torchtext) (1.22)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from torch==1.8.1->torchtext) (0.8)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from torch==1.8.1->torchtext) (3.7.4.3)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "twisted 20.3.0 has requirement attrs>=19.2.0, but you'll have attrs 18.1.0 which is incompatible.\n",
      "automat 20.2.0 has requirement attrs>=19.2.0, but you'll have attrs 18.1.0 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 21.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\OneDrive\\projects\\jupyter-notebook\\text_mining\\09-classifier\\.data\\train.csv: 29.5MB [00:01, 23.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train_iter = AG_NEWS(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = AG_NEWS(split='train')\n",
    "counter = Counter()\n",
    "for (label, line) in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[476, 22, 31, 5298]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[token] for token in ['here', 'is', 'an', 'example']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[476, 22, 3, 31, 5298]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('here is the an example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://pytorch.org/tutorials/_images/text_sentiment_ngrams_model.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 : World\n",
    "2 : Sports\n",
    "3 : Business\n",
    "4 : Sci/Tec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label = model(text, offsets)\n",
    "        loss = criterion(predited_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predited_label = model(text, offsets)\n",
    "            loss = criterion(predited_label, label)\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\OneDrive\\projects\\jupyter-notebook\\text_mining\\09-classifier\\.data\\test.csv: 1.86MB [00:00, 4.65MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1782 batches | accuracy    0.679\n",
      "| epoch   1 |  1000/ 1782 batches | accuracy    0.857\n",
      "| epoch   1 |  1500/ 1782 batches | accuracy    0.875\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 18.55s | valid accuracy    0.886 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1782 batches | accuracy    0.895\n",
      "| epoch   2 |  1000/ 1782 batches | accuracy    0.902\n",
      "| epoch   2 |  1500/ 1782 batches | accuracy    0.902\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 35.89s | valid accuracy    0.903 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1782 batches | accuracy    0.915\n",
      "| epoch   3 |  1000/ 1782 batches | accuracy    0.915\n",
      "| epoch   3 |  1500/ 1782 batches | accuracy    0.914\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 35.02s | valid accuracy    0.907 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1782 batches | accuracy    0.925\n",
      "| epoch   4 |  1000/ 1782 batches | accuracy    0.924\n",
      "| epoch   4 |  1500/ 1782 batches | accuracy    0.921\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 34.56s | valid accuracy    0.897 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1782 batches | accuracy    0.936\n",
      "| epoch   5 |  1000/ 1782 batches | accuracy    0.938\n",
      "| epoch   5 |  1500/ 1782 batches | accuracy    0.938\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 36.01s | valid accuracy    0.914 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1782 batches | accuracy    0.939\n",
      "| epoch   6 |  1000/ 1782 batches | accuracy    0.938\n",
      "| epoch   6 |  1500/ 1782 batches | accuracy    0.938\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 35.21s | valid accuracy    0.914 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1782 batches | accuracy    0.941\n",
      "| epoch   7 |  1000/ 1782 batches | accuracy    0.940\n",
      "| epoch   7 |  1500/ 1782 batches | accuracy    0.942\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 34.50s | valid accuracy    0.915 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1782 batches | accuracy    0.941\n",
      "| epoch   8 |  1000/ 1782 batches | accuracy    0.939\n",
      "| epoch   8 |  1500/ 1782 batches | accuracy    0.940\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 35.94s | valid accuracy    0.916 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1782 batches | accuracy    0.940\n",
      "| epoch   9 |  1000/ 1782 batches | accuracy    0.939\n",
      "| epoch   9 |  1500/ 1782 batches | accuracy    0.940\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 33.67s | valid accuracy    0.915 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1782 batches | accuracy    0.940\n",
      "| epoch  10 |  1000/ 1782 batches | accuracy    0.939\n",
      "| epoch  10 |  1500/ 1782 batches | accuracy    0.941\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 20.56s | valid accuracy    0.915 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = list(train_iter)\n",
    "test_dataset = list(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sci/Tec news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
