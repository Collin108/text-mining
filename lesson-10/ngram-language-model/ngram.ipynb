{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "SOS = \"<s> \"\n",
    "EOS = \"</s>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "def add_sentence_tokens(sentences, n):\n",
    "    \"\"\"Wrap each sentence in SOS and EOS tokens.\n",
    "\n",
    "    For n >= 2, n-1 SOS tokens are added, otherwise only one is added.\n",
    "\n",
    "    Args:\n",
    "        sentences (list of str): the sentences to wrap.\n",
    "        n (int): order of the n-gram model which will use these sentences.\n",
    "    Returns:\n",
    "        List of sentences with SOS and EOS tokens wrapped around them.\n",
    "\n",
    "    \"\"\"\n",
    "    sos = SOS * (n-1) if n > 1 else SOS\n",
    "    return ['{}{} {}'.format(sos, s, EOS) for s in sentences]\n",
    "\n",
    "def replace_singletons(tokens):\n",
    "    \"\"\"Replace tokens which appear only once in the corpus with <UNK>.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list of str): the tokens comprising the corpus.\n",
    "    Returns:\n",
    "        The same list of tokens with each singleton replaced by <UNK>.\n",
    "    \n",
    "    \"\"\"\n",
    "    vocab = nltk.FreqDist(tokens)\n",
    "    return [token if vocab[token] > 1 else UNK for token in tokens]\n",
    "\n",
    "def preprocess(sentences, n):\n",
    "    \"\"\"Add SOS/EOS/UNK tokens to given sentences and tokenize.\n",
    "\n",
    "    Args:\n",
    "        sentences (list of str): the sentences to preprocess.\n",
    "        n (int): order of the n-gram model which will use these sentences.\n",
    "    Returns:\n",
    "        The preprocessed sentences, tokenized by words.\n",
    "\n",
    "    \"\"\"\n",
    "    sentences = add_sentence_tokens(sentences, n)\n",
    "    tokens = ' '.join(sentences).split(' ')\n",
    "    tokens = replace_singletons(tokens)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import argparse\n",
    "from itertools import product\n",
    "import math\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "\n",
    "from preprocess import preprocess\n",
    "\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\"Load train and test corpora from a directory.\n",
    "\n",
    "    Directory must contain two files: train.txt and test.txt.\n",
    "    Newlines will be stripped out. \n",
    "\n",
    "    Args:\n",
    "        data_dir (Path) -- pathlib.Path of the directory to use. \n",
    "\n",
    "    Returns:\n",
    "        The train and test sets, as lists of sentences.\n",
    "\n",
    "    \"\"\"\n",
    "    train_path = data_dir.joinpath('train.txt').absolute().as_posix()\n",
    "    test_path  = data_dir.joinpath('test.txt').absolute().as_posix()\n",
    "\n",
    "    with open(train_path, 'r') as f:\n",
    "        train = [l.strip() for l in f.readlines()]\n",
    "    with open(test_path, 'r') as f:\n",
    "        test = [l.strip() for l in f.readlines()]\n",
    "    return train, test\n",
    "\n",
    "\n",
    "class LanguageModel(object):\n",
    "    \"\"\"An n-gram language model trained on a given corpus.\n",
    "    \n",
    "    For a given n and given training corpus, constructs an n-gram language\n",
    "    model for the corpus by:\n",
    "    1. preprocessing the corpus (adding SOS/EOS/UNK tokens)\n",
    "    2. calculating (smoothed) probabilities for each n-gram\n",
    "\n",
    "    Also contains methods for calculating the perplexity of the model\n",
    "    against another corpus, and for generating sentences.\n",
    "\n",
    "    Args:\n",
    "        train_data (list of str): list of sentences comprising the training corpus.\n",
    "        n (int): the order of language model to build (i.e. 1 for unigram, 2 for bigram, etc.).\n",
    "        laplace (int): lambda multiplier to use for laplace smoothing (default 1 for add-1 smoothing).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_data, n, laplace=1):\n",
    "        self.n = n\n",
    "        self.laplace = laplace\n",
    "        self.tokens = preprocess(train_data, n)\n",
    "        self.vocab  = nltk.FreqDist(self.tokens)\n",
    "        self.model  = self._create_model()\n",
    "        self.masks  = list(reversed(list(product((0,1), repeat=n))))\n",
    "\n",
    "    def _smooth(self):\n",
    "        \"\"\"Apply Laplace smoothing to n-gram frequency distribution.\n",
    "        \n",
    "        Here, n_grams refers to the n-grams of the tokens in the training corpus,\n",
    "        while m_grams refers to the first (n-1) tokens of each n-gram.\n",
    "\n",
    "        Returns:\n",
    "            dict: Mapping of each n-gram (tuple of str) to its Laplace-smoothed \n",
    "            probability (float).\n",
    "\n",
    "        \"\"\"\n",
    "        vocab_size = len(self.vocab)\n",
    "\n",
    "        n_grams = nltk.ngrams(self.tokens, self.n)\n",
    "        n_vocab = nltk.FreqDist(n_grams)\n",
    "\n",
    "        m_grams = nltk.ngrams(self.tokens, self.n-1)\n",
    "        m_vocab = nltk.FreqDist(m_grams)\n",
    "\n",
    "        def smoothed_count(n_gram, n_count):\n",
    "            m_gram = n_gram[:-1]\n",
    "            m_count = m_vocab[m_gram]\n",
    "            return (n_count + self.laplace) / (m_count + self.laplace * vocab_size)\n",
    "\n",
    "        return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n",
    "\n",
    "    def _create_model(self):\n",
    "        \"\"\"Create a probability distribution for the vocabulary of the training corpus.\n",
    "        \n",
    "        If building a unigram model, the probabilities are simple relative frequencies\n",
    "        of each token with the entire corpus.\n",
    "\n",
    "        Otherwise, the probabilities are Laplace-smoothed relative frequencies.\n",
    "\n",
    "        Returns:\n",
    "            A dict mapping each n-gram (tuple of str) to its probability (float).\n",
    "\n",
    "        \"\"\"\n",
    "        if self.n == 1:\n",
    "            num_tokens = len(self.tokens)\n",
    "            return { (unigram,): count / num_tokens for unigram, count in self.vocab.items() }\n",
    "        else:\n",
    "            return self._smooth()\n",
    "\n",
    "    def _convert_oov(self, ngram):\n",
    "        \"\"\"Convert, if necessary, a given n-gram to one which is known by the model.\n",
    "\n",
    "        Starting with the unmodified ngram, check each possible permutation of the n-gram\n",
    "        with each index of the n-gram containing either the original token or <UNK>. Stop\n",
    "        when the model contains an entry for that permutation.\n",
    "\n",
    "        This is achieved by creating a 'bitmask' for the n-gram tuple, and swapping out\n",
    "        each flagged token for <UNK>. Thus, in the worst case, this function checks 2^n\n",
    "        possible n-grams before returning.\n",
    "\n",
    "        Returns:\n",
    "            The n-gram with <UNK> tokens in certain positions such that the model\n",
    "            contains an entry for it.\n",
    "\n",
    "        \"\"\"\n",
    "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n",
    "\n",
    "        ngram = (ngram,) if type(ngram) is str else ngram\n",
    "        for possible_known in [mask(ngram, bitmask) for bitmask in self.masks]:\n",
    "            if possible_known in self.model:\n",
    "                return possible_known\n",
    "\n",
    "    def perplexity(self, test_data):\n",
    "        \"\"\"Calculate the perplexity of the model against a given test corpus.\n",
    "        \n",
    "        Args:\n",
    "            test_data (list of str): sentences comprising the training corpus.\n",
    "        Returns:\n",
    "            The perplexity of the model as a float.\n",
    "        \n",
    "        \"\"\"\n",
    "        test_tokens = preprocess(test_data, self.n)\n",
    "        test_ngrams = nltk.ngrams(test_tokens, self.n)\n",
    "        N = len(test_tokens)\n",
    "\n",
    "        known_ngrams  = (self._convert_oov(ngram) for ngram in test_ngrams)\n",
    "        probabilities = [self.model[ngram] for ngram in known_ngrams]\n",
    "\n",
    "        return math.exp((-1/N) * sum(map(math.log, probabilities)))\n",
    "\n",
    "    def _best_candidate(self, prev, i, without=[]):\n",
    "        \"\"\"Choose the most likely next token given the previous (n-1) tokens.\n",
    "\n",
    "        If selecting the first word of the sentence (after the SOS tokens),\n",
    "        the i'th best candidate will be selected, to create variety.\n",
    "        If no candidates are found, the EOS token is returned with probability 1.\n",
    "\n",
    "        Args:\n",
    "            prev (tuple of str): the previous n-1 tokens of the sentence.\n",
    "            i (int): which candidate to select if not the most probable one.\n",
    "            without (list of str): tokens to exclude from the candidates list.\n",
    "        Returns:\n",
    "            A tuple with the next most probable token and its corresponding probability.\n",
    "\n",
    "        \"\"\"\n",
    "        blacklist  = [\"<UNK>\"] + without\n",
    "        candidates = ((ngram[-1],prob) for ngram,prob in self.model.items() if ngram[:-1]==prev)\n",
    "        candidates = filter(lambda candidate: candidate[0] not in blacklist, candidates)\n",
    "        candidates = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)\n",
    "        if len(candidates) == 0:\n",
    "            return (\"</s>\", 1)\n",
    "        else:\n",
    "            return candidates[0 if prev != () and prev[-1] != \"<s>\" else i]\n",
    "     \n",
    "    def generate_sentences(self, num, min_len=12, max_len=24):\n",
    "        \"\"\"Generate num random sentences using the language model.\n",
    "\n",
    "        Sentences always begin with the SOS token and end with the EOS token.\n",
    "        While unigram model sentences will only exclude the UNK token, n>1 models\n",
    "        will also exclude all other words already in the sentence.\n",
    "\n",
    "        Args:\n",
    "            num (int): the number of sentences to generate.\n",
    "            min_len (int): minimum allowed sentence length.\n",
    "            max_len (int): maximum allowed sentence length.\n",
    "        Yields:\n",
    "            A tuple with the generated sentence and the combined probability\n",
    "            (in log-space) of all of its n-grams.\n",
    "\n",
    "        \"\"\"\n",
    "        for i in range(num):\n",
    "            sent, prob = [\"<s>\"] * max(1, self.n-1), 1\n",
    "            while sent[-1] != \"</s>\":\n",
    "                prev = () if self.n == 1 else tuple(sent[-(self.n-1):])\n",
    "                blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])\n",
    "                next_token, next_prob = self._best_candidate(prev, i, without=blacklist)\n",
    "                sent.append(next_token)\n",
    "                prob *= next_prob\n",
    "                \n",
    "                if len(sent) >= max_len:\n",
    "                    sent.append(\"</s>\")\n",
    "\n",
    "            yield ' '.join(sent), -1/math.log(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总句子数： 44473\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "punctions = ['！','。','?','']\n",
    "with open('../../lesson-7/199801.txt',\"r\",encoding=\"utf-8\") as f:\n",
    "    #c = 0\n",
    "    for line in f:\n",
    "        #if c>10000:\n",
    "        #    break\n",
    "        #c+=1\n",
    "        ls = line.strip().split('  ')\n",
    "        line_segs = []\n",
    "        candidate_sentence = []\n",
    "        w_num = 0\n",
    "        for i in range(1,len(ls)):\n",
    "            if ls[i].endswith('/w') and ls[i][0] in punctions:\n",
    "                w_num+=1\n",
    "                if len(candidate_sentence)>=1:\n",
    "                    sentences.append(candidate_sentence)\n",
    "                    candidate_sentence = []\n",
    "            else:\n",
    "                end_idx = ls[i].index('/')\n",
    "                candidate_sentence.append(ls[i][:end_idx])\n",
    "        if len(candidate_sentence)>0:\n",
    "            sentences.append(candidate_sentence)\n",
    "print(\"总句子数：\",len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(\"N-gram Language Model\")\n",
    "#     parser.add_argument('--data', type=str, required=True,\n",
    "#             help='Location of the data directory containing train.txt and test.txt')\n",
    "#     parser.add_argument('--n', type=int, required=True,\n",
    "#             help='Order of N-gram model to create (i.e. 1 for unigram, 2 for bigram, etc.)')\n",
    "#     parser.add_argument('--laplace', type=float, default=0.01,\n",
    "#             help='Lambda parameter for Laplace smoothing (default is 0.01 -- use 1 for add-1 smoothing)')\n",
    "#     parser.add_argument('--num', type=int, default=10,\n",
    "#             help='Number of sentences to generate (default 10)')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # Load and prepare train/test data\n",
    "    # data_path = Path(args.data)\n",
    "    train, test = load_data(data_path)\n",
    "\n",
    "    print(\"Loading {}-gram model...\".format(args.n))\n",
    "    lm = LanguageModel(train, args.n, laplace=args.laplace)\n",
    "    print(\"Vocabulary size: {}\".format(len(lm.vocab)))\n",
    "\n",
    "    print(\"Generating sentences...\")\n",
    "    for sentence, prob in lm.generate_sentences(args.num):\n",
    "        print(\"{} ({:.5f})\".format(sentence, prob))\n",
    "\n",
    "    perplexity = lm.perplexity(test)\n",
    "    print(\"Model perplexity: {:.3f}\".format(perplexity))\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepwide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
